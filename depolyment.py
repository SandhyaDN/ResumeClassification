# -*- coding: utf-8 -*-
"""depolyment

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1NXo0N1r6s49WUhYhFWqCE1-3yKnIqTtq
"""

pip install streamlit tika transformers torch

from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import LabelEncoder
import pandas as pd
import joblib

# clean_text function
def clean_text(text):
    if isinstance(text, str):
        return text.lower()  # Example: convert to lowercase
    return ""


df = pd.read_csv("resumes_dataset.csv")

x = df['text']
y = df['label']

# Train/Test split
x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)

# Pipeline: TF-IDF + Logistic Regression
pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features = None, preprocessor = clean_text)),
    ('logreg', LogisticRegression(random_state = 42, multi_class = 'multinomial'))
])
# creating new x, y for pipeline usage
x_pipe = df['text']
label_en = LabelEncoder()
y_pipe = label_en.fit_transform(df['label'])
x_train_pipe, x_test_pipe, y_train_pipe, y_test_pipe = train_test_split(x_pipe, y_pipe, test_size = 0.2,
                                                                    random_state = 42, stratify = y_pipe)

# Fit pipeline
pipeline.fit(x_train_pipe, y_train_pipe)

# Save the pipeline
joblib.dump(pipeline, 'resume_pipeline.pkl')

import streamlit as st
from tika import parser
from transformers import pipeline

# Load Hugging Face zero-shot classifier
@st.cache_resource
def load_model():
    return pipeline("zero-shot-classification", model="facebook/bart-large-mnli")

classifier = load_model()

# Function to extract text from resume
def extract_text(uploaded_file):
    # Save uploaded file temporarily
    with open("temp_file", "wb") as f:
        f.write(uploaded_file.getbuffer())
    parsed = parser.from_file("temp_file")
    return parsed.get("content", "")

# Streamlit UI
st.title("üìÑ Resume Classification")

uploaded_file = st.file_uploader("Upload Resume", type=["pdf", "docx", "doc"])

if uploaded_file is not None:
    resume_text = extract_text(uploaded_file)

    if resume_text.strip() == "":
        st.error("‚ö†Ô∏è Could not extract text from the file. Try another resume.")
    else:
        st.subheader("Extracted Resume Text:")
        st.write(resume_text[:1000])  # Display first 1000 characters

        # Define categories
        categories = ["Data Scientist", "Software Engineer", "Teacher", "Accountant", "HR", "Manager"]

        # Run classification
        result = classifier(resume_text, candidate_labels=categories)

        st.subheader("Predicted Category")
        st.success(result['labels'][0])

        # Show all probabilities
        st.subheader("Prediction Confidence")
        for label, score in zip(result['labels'], result['scores']):
            st.write(f"{label}: {score:.2f}")